# Burmese BERT Model with SentencePiece Encoding

Bert for Burmese Masked Language Modeling  (MLM) Task

## Notebook

**bert_burmese_sentencepiece.ipynb**
   - This notebook demonstrates the process of data preparation, training, and testing the Burmese BERT model using SentencePiece encoding.

## Instructions

### Requirements
- Python 3.x
- SentencePiece
- Datasets
- Transformers

### Data Preparation

The data preparation for the Burmese language dataset is already coded in the notebook. Ensure that you have your Burmese dataset ready in a format suitable for BERT training. If you need to customize or adapt the data preparation steps, refer to the data preparation section within the notebook for any adjustments.

### Steps to Reproduce

1. **Notebook Execution**
   - Open and run the `bert_burmese_sentencepiece.ipynb` notebook.
   - Follow the instructions within the notebook for model training and evaluation.

2. **Model Testing**
   - Use the provided testing section in the notebook to evaluate the model's performance.

3. **Adjust Parameters**
   - Customize the hyperparameters and model configurations as needed.

### Acknowledgments

- [https://towardsdatascience.com/how-to-train-a-bert-model-from-scratch-72cfce554fc6]

Feel free to reach out if you have any questions or issues!

