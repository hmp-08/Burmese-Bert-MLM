# Burmese BERT Models with SentencePiece Encoding

Bert on MLM and NSP tasks

## Notebooks

**1. bert_burmese_sentencepiece.ipynb**
   - This notebook demonstrates the process of data preparation, training, and testing the Burmese BERT model using SentencePiece encoding for both MLM (Masked Language Model) task.

**2. bert_burmese_for_nsp.ipynb**
   - This notebook focuses specifically on training the Burmese BERT model for the NSP (Next Sentence Prediction) task. It covers data preparation, model training, and testing related to NSP.

## Instructions

### Requirements
- Python 3.x
- SentencePiece
- Datasets
- Transformers

### Steps to Reproduce

1. **Notebook Execution**
   - Open and run the respective notebooks (`bert_burmese_sentencepiece.ipynb` and `bert_burmese_for_nsp.ipynb`).
   - Follow the instructions within each notebook for model training and evaluation.

2. **Model Testing**
   - Use the provided testing sections in the notebooks to evaluate the model's performance.

3. **Adjust Parameters**
   - Customize the hyperparameters and model configurations as needed.

### Acknowledgments

- [https://towardsdatascience.com/how-to-train-a-bert-model-from-scratch-72cfce554fc6]

Feel free to reach out if you have any questions or issues!
